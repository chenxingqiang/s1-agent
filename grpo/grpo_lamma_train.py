# -*- coding: utf-8 -*-
"""Llama3.1_(8B)-GRPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb

To run this, press "*Runtime*" and press "*Run all*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
<a href="https://unsloth.ai/"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>

To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).

You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)

### News

**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**

Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).

### Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Skip restarting message in Colab
# import sys; modules = list(sys.modules.keys())
# for x in modules: sys.modules.pop(x) if "PIL" in x or "google" in x else None
# 
# !pip install unsloth vllm
# !pip install --upgrade pillow

"""### Unsloth

Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!
"""

from unsloth import FastLanguageModel, PatchFastRL
PatchFastRL("GRPO", FastLanguageModel)

"""Load up `Llama 3.1 8B Instruct`, and set parameters"""

from unsloth import is_bfloat16_supported
import torch
max_seq_length = 512 # Can increase for longer reasoning traces
lora_rank = 32 # Larger rank = smarter, but slower

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "meta-llama/meta-Llama-3.1-8B-Instruct",
    max_seq_length = max_seq_length,
    load_in_4bit = True, # False for LoRA 16bit
    fast_inference = True, # Enable vLLM fast inference
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.6, # Reduce if out of memory
)

model = FastLanguageModel.get_peft_model(
    model,
    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ], # Remove QKVO if out of memory
    lora_alpha = lora_rank,
    use_gradient_checkpointing = "unsloth", # Enable long context finetuning
    random_state = 3407,
)

"""### Data Prep
<a name="Data"></a>

We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!
"""

import re
from datasets import load_dataset, Dataset
import os

# Load and prep dataset
SYSTEM_PROMPT = """
Provide a structured mathematical solution following this format:

<solution version="1.0">
    <problem_analysis>
        <given_information>
            [List all given values, conditions, and constraints]
        </given_information>
        <assumptions>
            [State any necessary assumptions]
        </assumptions>
        <target>
            [Clearly state what needs to be solved/proven]
        </target>
    </problem_analysis>

    <solution_approach>
        <method>
            [Name and justify the chosen solution method]
        </method>
        <key_concepts>
            [List relevant mathematical concepts/formulas]
        </key_concepts>
        <variables>
            [Define all variables used]
        </variables>
    </solution_approach>

    <mathematical_steps>
        <step number="1">
            [Initial step with explanation]
            <equation>{equation_1}</equation>
            <justification>{justification_1}</justification>
        </step>
        <step number="2">
            [Next step with explanation]
            <equation>{equation_2}</equation>
            <justification>{justification_2}</justification>
        </step>
        <!-- Additional steps as needed -->
    </mathematical_steps>

    <verification>
        <dimension_check>
            [Verify units/dimensions are consistent]
        </dimension_check>
        <boundary_check>
            [Verify answer makes sense within problem constraints]
        </boundary_check>
        <alternative_method>
            [Optional: Verify using different approach]
        </alternative_method>
    </verification>

    <final_answer>
        <numerical_result>
            [Final calculated value with units]
        </numerical_result>
        <interpretation>
            [Explain what this result means in context]
        </interpretation>
        <confidence_level>
            [High/Medium/Low with mathematical justification]
        </confidence_level>
    </final_answer>
</solution>

Note:
- Show all mathematical steps clearly
- Include units in calculations
- Justify each significant step
- Verify your answer makes sense
- Use proper mathematical notation
- Check boundary conditions
"""

MATH_MD_FORMAT = """\
# Problem Analysis
## Given Information
{givens}

## Assumptions
{assumptions}

## Target
$${target}$$

# Solution Approach
## Method
{method}

## Key Concepts
- Mathematical Concepts:
$${concepts}$$

## Variables
Let:
$${variables}$$

# Mathematical Steps
{steps}

# Verification
## Dimension Check
$${dimensions}$$

## Boundary Check
$${bounds}$$

## Alternative Method
$${alternative}$$

# Final Answer
## Numerical Result
$${result}$$

## Interpretation
{interpretation}

## Confidence Level
{confidence}
- Mathematical Justification: $${confidence_math}$$
"""

def validate_math_solution(text: str) -> bool:
    required_sections = [
        "# Problem Analysis",
        "## Given Information",
        "## Assumptions", 
        "# Solution Approach",
        "# Mathematical Steps",
        "# Verification",
        "# Final Answer"
    ]
    return all(section in text for section in required_sections)

def math_structure_reward_func(completions, **kwargs) -> list[float]:
    responses = [completion[0]["content"] for completion in completions]
    rewards = []
    for response in responses:
        reward = 0.0
        # Check for proper mathematical structure
        if validate_math_solution(response):
            reward += 0.3
        # Check for equations
        if "$$" in response:
            reward += 0.2
        # Check for verification sections
        if "## Dimension Check" in response and "## Boundary Check" in response:
            reward += 0.2
        # Check for proper variable definitions
        if "## Variables" in response and "## Key Concepts" in response:
            reward += 0.2
        rewards.append(reward)
    return rewards

def extract_md_answer(text: str) -> str:
    try:
        # First try to extract the numerical result
        if "## Numerical Result" in text:
            result_section = text.split("## Numerical Result")[1]
            result = result_section.split("##")[0]
            # Extract content between $$ if present
            if "$$" in result:
                result = result.split("$$")[1]
            return result.strip()
        
        # Fall back to the entire Final Answer section
        if "# Final Answer" in text:
            answer_section = text.split("# Final Answer")[1]
            if "# " in answer_section:
                answer_section = answer_section.split("# ")[0]
            return answer_section.strip()
    except:
        return text.strip()

def extract_hash_answer(text: str) -> str | None:
    if "####" not in text:
        return None
    return text.split("####")[1].strip()

# uncomment middle messages for 1-shot prompting
def get_gsm8k_questions(split = "train") -> Dataset:
    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore
    data = data.map(lambda x: { # type: ignore
        'prompt': [
            {'role': 'system', 'content': SYSTEM_PROMPT},
            {'role': 'user', 'content': x['question']}
        ],
        'answer': extract_hash_answer(x['answer'])
    }) # type: ignore
    return data # type: ignore

dataset = get_gsm8k_questions()

# Reward functions
def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    q = prompts[0][-1]['content']
    extracted_responses = [extract_md_answer(r) for r in responses]
    print('-'*20, f"Question:\n{q}", f"\nAnswer:\n{answer[0]}", f"\nResponse:\n{responses[0]}", f"\nExtracted:\n{extracted_responses[0]}")
    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]

def int_reward_func(completions, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    extracted_responses = [extract_md_answer(r) for r in responses]
    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]

def strict_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks if the completion follows the exact MD format."""
    pattern = (
        r"# Problem Analysis\s*"
        r"## Given Information.*?"
        r"## Assumptions.*?"
        r"## Target.*?"
        r"# Solution Approach\s*"
        r"## Method.*?"
        r"## Key Concepts.*?"
        r"## Variables.*?"
        r"# Mathematical Steps.*?"
        r"# Verification\s*"
        r"## Dimension Check.*?"
        r"## Boundary Check.*?"
        r"## Alternative Method.*?"
        r"# Final Answer\s*"
        r"## Numerical Result.*?"
        r"## Interpretation.*?"
        r"## Confidence Level.*?"
    )
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r, re.DOTALL) for r in responses]
    return [0.5 if match else 0.0 for match in matches]

def soft_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks for presence of main MD sections in any order."""
    required_sections = [
        r"# Problem Analysis",
        r"# Solution Approach",
        r"# Mathematical Steps",
        r"# Verification",
        r"# Final Answer"
    ]
    responses = [completion[0]["content"] for completion in completions]
    rewards = []
    for response in responses:
        matches = [re.search(pattern, response, re.DOTALL) is not None 
                  for pattern in required_sections]
        reward = 0.5 * (sum(matches) / len(required_sections))
        rewards.append(reward)
    return rewards

def count_md(text) -> float:
    count = 0.0
    main_sections = [
        ("# Problem Analysis", 0.125),
        ("# Solution Approach", 0.125),
        ("# Mathematical Steps", 0.125),
        ("# Verification", 0.125),
        ("# Final Answer", 0.125)
    ]
    
    for section, weight in main_sections:
        if section in text:
            count += weight
    
    # Check for mathematical elements
    if "$$" in text:
        count += 0.125
    
    # Check for subsections
    subsections = [
        "## Given Information",
        "## Assumptions",
        "## Method",
        "## Variables",
        "## Dimension Check",
        "## Boundary Check",
        "## Numerical Result"
    ]
    for subsection in subsections:
        if subsection in text:
            count += 0.025
    
    # Penalize content after Final Answer
    if "# Final Answer" in text:
        final_section = text.split("# Final Answer")[1]
        if "# " in final_section:
            count -= 0.001 * len(final_section.split("# ")[1])
    
    return min(count, 1.0)

def xmlcount_reward_func(completions, **kwargs) -> list[float]:
    contents = [completion[0]["content"] for completion in completions]
    return [count_md(c) for c in contents]

"""<a name="Train"></a>
### Train the model

Now set up GRPO Trainer and all configurations!
"""

from trl import GRPOConfig, GRPOTrainer
import wandb

# Initialize wandb
wandb.init(project="Llama3.1-8B-R1-Math-Solution", entity="openmodels")

# Update training args to enable wandb
training_args = GRPOConfig(
    use_vllm = True,
    learning_rate = 5e-6,
    adam_beta1 = 0.9,
    adam_beta2 = 0.99,
    weight_decay = 0.1,
    warmup_ratio = 0.1,
    lr_scheduler_type = "cosine",
    optim = "paged_adamw_8bit",
    logging_steps = 1,
    bf16 = is_bfloat16_supported(),
    fp16 = not is_bfloat16_supported(),
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 1,
    num_generations = 6,
    max_prompt_length = 256,
    max_completion_length = 200,
    max_steps = 250,
    save_steps = 250,
    max_grad_norm = 0.1,
    report_to = "wandb",
    output_dir = "outputs/grpo_lamma_train"
)

"""And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!

You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!

| Step | Training Loss | reward    | reward_std | completion_length | kl       |
|------|---------------|-----------|------------|-------------------|----------|
| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |
| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |
| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |

"""

trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [
        xmlcount_reward_func,
        soft_format_reward_func,
        strict_format_reward_func,
        int_reward_func,
        correctness_reward_func,
        math_structure_reward_func,
    ],
    args = training_args,
    train_dataset = dataset,
)
trainer.train()

# Push to HuggingFace in multiple formats
if True: model.push_to_hub_merged("xingqiang/Llama3.1-8B-R1-Math-Solution", tokenizer, save_method = "merged_16bit", token = os.environ.get("HF_TOKEN"))

# Save to GGUF first before 4-bit
if True: model.push_to_hub_gguf(
    "xingqiang/Llama3.1-8B-R1-Math-Solution",
    tokenizer,
    quantization_method = ["q4_k_m", "q8_0", "q5_k_m"],
    token = os.environ.get("HF_TOKEN"),
)

# Now save 4-bit with forced flag
if True: model.push_to_hub_merged("xingqiang/Llama3.1-8B-R1-Math-Solution", tokenizer, save_method = "merged_4bit_forced", token = os.environ.get("HF_TOKEN"))

# Finish wandb run
wandb.finish()

"""<a name="Inference"></a>
### Inference
Now let's try the model we just trained! First, let's first try the model without any GRPO trained:
"""

text = tokenizer.apply_chat_template([
    {"role" : "user", "content" : "Calculate pi."},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 1024,
)
output = model.fast_generate(
    [text],
    sampling_params = sampling_params,
    lora_request = None,
)[0].outputs[0].text

output

"""And now with the LoRA we just trained with GRPO - we first save the LoRA first!"""

model.save_lora("grpo_saved_lora")

"""Now we load the LoRA and test:"""

text = tokenizer.apply_chat_template([
    {"role" : "system", "content" : SYSTEM_PROMPT},
    {"role" : "user", "content" : "Calculate pi."},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 1024,
)
output = model.fast_generate(
    text,
    sampling_params = sampling_params,
    lora_request = model.load_lora("grpo_saved_lora"),
)[0].outputs[0].text

output

"""Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!

<a name="Save"></a>
### Saving to float16 for VLLM

We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.
"""

# Merge to 16bit
if True: model.save_pretrained_merged("Llama3.1-8B-R1-Math-Solution-16bit", tokenizer, save_method = "merged_16bit",)
if True: model.push_to_hub_merged("xingqiang/Llama3.1-8B-R1-Math-Solution", tokenizer, save_method = "merged_16bit", token = os.environ.get("HF_TOKEN"))

# Merge to 4bit
if True: model.save_pretrained_merged("Llama3.1-8B-R1-Math-Solution-4bit", tokenizer, save_method = "merged_4bit",)
if True: model.push_to_hub_merged("xingqiang/Llama3.1-8B-R1-Math-Solution", tokenizer, save_method = "merged_4bit", token = os.environ.get("HF_TOKEN"))

# Just LoRA adapters
if True: model.save_pretrained_merged("Llama3.1-8B-R1-Math-Solution-LoRA", tokenizer, save_method = "lora",)
if True: model.push_to_hub_merged("xingqiang/Llama3.1-8B-R1-Math-Solution-LoRA", tokenizer, save_method = "lora", token = os.environ.get("HF_TOKEN"))

"""### GGUF / llama.cpp Conversion
To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.

Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):
* `q8_0` - Fast conversion. High resource use, but generally acceptable.
* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.
* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.

[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)
"""

# Save to 8bit Q8_0
if True: model.save_pretrained_gguf("Llama3.1-8B-R1-Math-Solution-8bit", tokenizer,)
if True: model.push_to_hub_gguf("xingqiang/Llama3.1-8B-R1-Math-Solution-8bit", tokenizer, token = os.environ.get("HF_TOKEN"))

# Save to 16bit GGUF
if True: model.save_pretrained_gguf("Llama3.1-8B-R1-Math-Solution-16bit", tokenizer, quantization_method = "f16")
if True: model.push_to_hub_gguf("xingqiang/Llama3.1-8B-R1-Math-Solution-16bit", tokenizer, quantization_method = "f16", token = os.environ.get("HF_TOKEN"))

# Save to q4_k_m GGUF
if True: model.save_pretrained_gguf("Llama3.1-8B-R1-Math-Solution-q4_k_m", tokenizer, quantization_method = "q4_k_m")
if True: model.push_to_hub_gguf("xingqiang/Llama3.1-8B-R1-Math-Solution-q4_k_m", tokenizer, quantization_method = "q4_k_m", token = os.environ.get("HF_TOKEN"))

# Save to multiple GGUF options - much faster if you want multiple!
if True:
    model.push_to_hub_gguf(
        "xingqiang/Llama3.1-8B-R1-Math-Solution", # Change hf to your username!
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = os.environ.get("HF_TOKEN"),
    )

"""Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)

And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!

Some other links:
1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)
2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)
3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)
6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!

<div class="align-center">
  <a href="https://unsloth.ai"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord.png" width="145"></a>
  <a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a>

  Join Discord if you need help + ⭐️ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐️
</div>

"""

import os
import sys
import tempfile
import time
from pathlib import Path
import itertools

import tqdm

import torch
import torch.nn.functional as F
from torch.utils.data.distributed import DistributedSampler

import torch._inductor.config
import torch._dynamo.config

torch._inductor.config.coordinate_descent_tuning = True
torch._inductor.config.triton.unique_kernel_names = True
torch._inductor.config.fx_graph_cache = True  # Experimental feature to reduce compilation times, will be on by default in future

torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

import wandb

# support running without installing as a package
wd = Path(__file__).parent.parent.resolve()
sys.path.append(str(wd))